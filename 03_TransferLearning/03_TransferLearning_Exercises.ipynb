{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Transfer Learning Exercises",
   "metadata": {
    "id": "2PYOnt_yJkaI",
    "colab_type": "text",
    "cell_id": "00000-91035967-1f6d-4cda-a4fc-ae720c12bcf7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DNCmsEpLJkaJ",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00001-2d03c604-2478-4fd5-b130-5e2ce13d4f13",
    "deepnote_cell_type": "code"
   },
   "source": "# Import useful libraries        (note: don't forget to turn on GPU)\n\n# tensorflow for network building/training\nimport tensorflow as tf\nfrom tensorflow.python.keras import Model, Sequential\nfrom tensorflow.keras.applications import VGG16\n\n# Basic operating system (os), numerical, and plotting functionality\nimport os\nimport time\nimport numpy as np\nfrom matplotlib import pylab as plt\n\n# scikit-learn data utilities\nfrom sklearn.model_selection import train_test_split\nfrom skimage import transform\n\n# scikit-learn performance metric utilities\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# Color transformations\nfrom skimage.color import rgb2lab\n\n#Skimage resizing \nfrom skimage.transform import resize\n\n# Garbage collection (for saving RAM during training)\nimport gc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## VGG16 Model\n\nFor this exercise you'll now use the VGG16 model as the feature extractor. https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16\n\nSpecifications:\n- Default input size: 224x224, no smaller than 32x32 pixels\n- Default output classes: 1000\n\nOur images are 150x150 pixels in size and come from only **eight categories**. In order to use this model for our classification task, we again can/need to do the following:\n* Resize images : Our input images can be resized to the appropriate dimensions. Alternatively, we can pad our images to the expected dimensions. Padding leads to additional choices - Do we pad with zeros, duplicate edge pixels or mirror the image across edges?\n* Change the prediction layer : Remove the existing prediction layer and add a new layer that can predict **8 classes**.\n* Train : Finally, we need to train the network on our data",
   "metadata": {
    "id": "1IkUE60WJkaQ",
    "colab_type": "text",
    "cell_id": "00002-2a1aa0d4-6e94-4033-8aa2-fcea42d2566e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Load Data",
   "metadata": {
    "id": "teogiXA3Jkak",
    "colab_type": "text",
    "cell_id": "00003-846be333-a2dd-49bf-8139-f7a97c1ce332",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Getting path and changing directories",
   "metadata": {
    "id": "AwwkYk48Jkal",
    "colab_type": "text",
    "cell_id": "00004-01c2d56a-c4f8-452d-b4fa-9d223a481092",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hYdSaY2PJkam",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00005-5ef20a32-3708-489b-81b2-fd256412e20e",
    "deepnote_cell_type": "code"
   },
   "source": "# Define the current directory and the directory where the files to download can\n# be found\ncurrent_dir = os.getcwd()\nremote_path = 'https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/'\n\n# Define and build a directory to save this data in\ndata_dir = os.path.join(current_dir, 'crc_data')\nif not os.path.isdir(data_dir):\n  os.mkdir(data_dir)\n\n# Move into the data directory and download all of the files\nos.chdir(data_dir)\nfor ii in range(1, 6):\n    basename = f'rgb0{ii}.npz'\n    filename = os.path.join(remote_path, basename)\n\n    # Check if the file has already been downloaded\n    if not os.path.isfile(basename):\n      cmd = f'wget {filename}'\n      print(cmd)\n      os.system(cmd)\n\n# Return to the original directory\nos.chdir(current_dir)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Function for loading images",
   "metadata": {
    "id": "NoYrP7JXJkat",
    "colab_type": "text",
    "cell_id": "00006-6dbe7aae-4274-4a72-8827-02557d9e1ad1",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1sesRgeVJkau",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00007-34e759b8-5a15-495c-9c20-634afa7f6e23",
    "deepnote_cell_type": "code"
   },
   "source": "# Define a function to load the data from the assumed download path\ndef load_images(colorspace='rgb'):\n    \"\"\"\n    Loads the example data and applies transformation into requested colorspace\n\n    Arguments\n    ---------\n    colorspace : str, optional, default: `rgb`\n        The colorspace into which the images should be transformed. Accepted\n        values include\n\n        'rgb' : Standard red-green-blue color-space for digital images\n\n        'gray' or 'grey': An arithmetic average of the (r, g, b) values\n\n        'lab': The CIE L*a*b* colorspace\n    \n    Returns\n    -------\n    images : numpy.ndarray, shape (Nimg, Ny, Nx, Ncolor)\n        The complete set of transformed images\n\n    labels : numpy.ndarray, shape (Nimg)\n        The classification labels associated with each entry in `images`\n\n    label_to_str : dict\n        A dictionary which converts the numerical classification value in\n        `labels` into its string equivalent representation.\n    \"\"\"\n    # Check that the colorspace argument is recognized\n    colorspace_lower = colorspace.lower()\n    if colorspace_lower not in ['rgb', 'gray', 'grey', 'lab']:\n        raise ValueError(f'`colorspace` value of {colorspace} not recognized')\n\n    # Load data, which is stored as a numpy archive file (.npz)\n    filename = os.path.join(data_dir, 'rgb01.npz')\n    print(f'loading {filename}')\n    tmp = np.load(os.path.join(data_dir, 'rgb01.npz'), allow_pickle=True)\n\n    # Parse the loaded data into images and labels\n    # Initialize the images and labels variables using the first archive data\n    images = tmp['rgb_data']\n    if colorspace_lower == 'rgb':\n        pass\n    elif colorspace_lower in ['gray', 'grey']:\n        images = np.mean(images, axis=-1)      # Average into grayscale\n    elif colorspace_lower == 'lab':\n        images = rgb2lab(images)               # Convert to CIE L*a*b*\n\n    # Grab the initial array for the image labels\n    labels = tmp['labels']\n    \n    # Grab the dictionary to convert numerical labels to their string equivalent\n    label_to_str = tmp['label_str']\n    label_to_str = label_to_str.tolist() # Convert label_to_str into a dict\n\n    # Update the user on the number and size of images loaded\n    print('Loaded images with shape {}'.format(images.shape))\n    del tmp\n\n    # Loop over each of the remaining archives and append the contained data\n    for ii in range(2,6):\n        # Build the full path to the archive and load it into memory\n        filename = os.path.join(data_dir, f'rgb0{ii}.npz')\n        print(f'loading {filename}')\n        tmp = np.load(filename, allow_pickle=True)\n\n        # Parse and append the data\n        these_images = tmp['rgb_data']\n        if colorspace_lower == 'rgb':\n            pass\n        elif (colorspace_lower == 'gray') or (colorspace_lower == 'grey'):\n            these_images = np.mean(these_images, axis=-1) # Convert to grayscale\n        elif colorspace_lower == 'lab':\n            these_images = rgb2lab(these_images)          # Convert to CIEL*a*b*\n\n        # Append the images and labels\n        images = np.append(images, these_images, axis=0)\n        labels = np.append(labels, tmp['labels'], axis=0)\n\n        # Update the user on the number and size of images\n        print('Loaded images with shape {}'.format(these_images.shape))\n        del tmp\n\n    # Force the image data to be floating point and print the data shape\n    images = images.astype(np.float)\n    print('Final image data shape: {}'.format(images.shape))\n    print('Number of image labels: {}'.format(*labels.shape))\n\n    return images, labels, label_to_str",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Load images and labels into memory",
   "metadata": {
    "id": "2gg4B0EOJka1",
    "colab_type": "text",
    "cell_id": "00008-c5098aca-8733-4fd6-9aeb-040173f048f4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YXzdtPccJka3",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00009-0b5975e4-ee31-48e6-a3b5-200a8c99f3a0",
    "deepnote_cell_type": "code"
   },
   "source": "images_full_res, labels, label_to_str = load_images()\nnum_classes = np.unique(labels).size",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Pre-process the Images\n\n***Note: you'll have to edit a line of code in the cell for resizing***",
   "metadata": {
    "id": "U8ri4n4KK3Ql",
    "colab_type": "text",
    "cell_id": "00010-e09d989c-d8ee-4424-adfa-194d1029743a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Resizing",
   "metadata": {
    "id": "FZqvdtsqJka7",
    "colab_type": "text",
    "cell_id": "00011-18100c99-b5c8-4985-8216-73b0f981b106",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6gAiiGUSJka7",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00012-ba4b0501-b1e6-4349-8dc0-de1f8339a0ce",
    "deepnote_cell_type": "code"
   },
   "source": "# This boolean can be switched to false if you do not want to resize the images\nresize_images_bool = True\n\n# Specify a new shape to use for the resized images\n# NOTE: For the VGG16 model, we must use a size of at least (32, 32).\noriginal_shape = images_full_res.shape\nnew_shape = list(original_shape)\nnew_shape[1:3] = ## YOUR CODE HERE\n\n# Compute if we are downsampling (in which case we need anti-aliasing)\nscaling_ratio = np.array(new_shape[1:3])/np.array(original_shape[1:3])\nanti_alias = np.any(scaling_ratio < 1)\n\n# If resizing is requested, then run the resizing transformation\nif resize_images_bool:\n    # Grab the original shape of the images\n    num_images = images_full_res.shape[0]\n\n    # Initialize an array for storing the resized images\n    images = np.zeros(new_shape, dtype=np.float16)\n\n    # Loop over each image in the data and perform a resizing operation\n    for img_num, img_data in enumerate(images_full_res):\n        # Update the user on progress\n        if np.mod(img_num, 1000) == 0:\n            print(f'Processing image number {img_num}')\n\n        # Process the image and force it to be a 16-bit float\n        processed_img = transform.resize(img_data, new_shape[1:],\n                                         anti_aliasing=anti_alias)\n        images[img_num] = processed_img.astype(np.float16)\n\n# If no resizing requested, then just rename that data\nelse:\n    images = images_full_res\n\n# Remove the full-resolution versions from memory (just clogging things up)\ndel images_full_res",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Normalize the images (if it hasn't been done already)",
   "metadata": {
    "id": "xQjuP1H1JkbE",
    "colab_type": "text",
    "cell_id": "00013-c841dc55-01a8-402c-a7ec-7640fbe1b36e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZE3wO4fKJkbF",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00014-cf06a3df-577c-4ced-bd0c-c37e1042ded3",
    "deepnote_cell_type": "code"
   },
   "source": "# Note, we cast image data as float16 to save RAM\nimages = images.astype(np.float16)/255.0",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Include an axis for color channels",
   "metadata": {
    "id": "dA8WJ25FTKjS",
    "colab_type": "text",
    "cell_id": "00015-91f62a92-07f0-4098-a36a-fa75f241577e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bNYIGee0TNK1",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00016-8056a91a-1fe7-41a7-bcc1-e649858b21af",
    "deepnote_cell_type": "code"
   },
   "source": "# Take note of number of color channels in the loaded image add a last axis to \n# images ndarray if array dimension is only 3 (as is the case with grayscale images)\nif images.ndim == 3:\n    # If image is grayscale, then we add a last axis (of len 1) for channel\n    n_channels = 1\n    images = images[:, : , :, np.newaxis]\n    print('\\nlast dimension added to images ndarray to account for channel')\n    print(f'new images.shape: {images.shape}')\nelse:\n    #if image is not grayscale, last dimension of image already corresponds to channel\n    n_channels = images.shape[-1]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Split data into train and test sets",
   "metadata": {
    "id": "rHV6NK27JkbA",
    "colab_type": "text",
    "cell_id": "00017-d548fabb-16e1-4ff8-b110-9cb1c89a1bcc",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fwmn7EuuJkbA",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00018-448c791b-8866-4304-b690-6c5390e6aefd",
    "deepnote_cell_type": "code"
   },
   "source": "# Split data into train and test sets\ntrain_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=.2)\n\n# Convert 'labels' (1D array of integers) to one-hot encoding\ntrain_labels = tf.keras.utils.to_categorical(train_labels)\ntest_labels = tf.keras.utils.to_categorical(test_labels)\n\n# Print sizes of train/test sets\nprint(f'train_images.shape: {train_images.shape}')\nprint(f'train_labels.shape: {train_labels.shape}')\nprint(f'test_images.shape: {test_images.shape}')\nprint(f'test_labels.shape: {test_labels.shape}')\n\n# Print the one-hot encoded labels as a sanity check\nprint('one-hot encoded labels:')\nprint(train_labels)\n\n# Get rid of the duplicate copies of the data\ndel images, labels",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Load Pre-trained VGG16 Model\n\nhere's the link to documentation again (https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16), also reference the tutorial notebook",
   "metadata": {
    "id": "4D9uhLlSJkbM",
    "colab_type": "text",
    "cell_id": "00019-2bac7120-ec9e-4219-be0a-be17493f9431",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Om5GOu21JkbO",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00020-28fd41fe-f615-4f1e-bee4-a1fe2c108083",
    "deepnote_cell_type": "code"
   },
   "source": "# Create the base pre-trained model\nprint('loading VGG16')\nbase_model = ## YOUR CODE HERE\nprint('done')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Summarize model structure",
   "metadata": {
    "id": "RhA1SR6VJkbR",
    "colab_type": "text",
    "cell_id": "00021-845a59b4-ce65-4020-b26c-7576687285dc",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5sQGrHQjJkbR",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00022-f5b7f47a-f81e-49f3-b9ea-15981948640f",
    "deepnote_cell_type": "code"
   },
   "source": "base_model.summary()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Freezing layers",
   "metadata": {
    "id": "zYNFl2ZOJkbZ",
    "colab_type": "text",
    "cell_id": "00023-37cbf075-b9a6-4b9d-be9c-158353bd2f2b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oLC3uT3cJkba",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00024-7ec76657-decf-4ece-83c9-b97649d5a01a",
    "deepnote_cell_type": "code"
   },
   "source": "# Play around with freezing layers, take a look at the tutorial notebook for reference \n\n# By default we'll just freeze the entire base model again\nbase_model.trainable = False",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Modify the pre-trained network by adding a few new layers at the output, including a classification layer (remember we want to predict 8 different classes)",
   "metadata": {
    "id": "wjn-7TTxJkbV",
    "colab_type": "text",
    "cell_id": "00025-5d246049-9efb-4ebb-9276-5c0563759d5e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-skRWsOAJkbW",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00026-17059d16-ad67-455a-ab70-edfa1c8f942f",
    "deepnote_cell_type": "code"
   },
   "source": "# Add a global spatial average pooling layer\n## YOUR CODE HERE\n\n# Add a fully-connected layer\n## YOUR CODE HERE\n\n# Add the final classification layer\n## YOUR CODE HERE\n\n# Build the model you will train\nmodel = ## YOUR CODE HERE\n\n# Print summary of model layers\nmodel.summary()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Compiling model",
   "metadata": {
    "id": "yvwvRkiVJkbd",
    "colab_type": "text",
    "cell_id": "00027-17137033-c484-4eeb-95af-c3e44c275fcc",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tC-_ZFi_Jkbe",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00028-5ed1b7a4-b3d4-40f8-ba44-deed0f922007",
    "deepnote_cell_type": "code"
   },
   "source": "# Compile the model (should be done *after* setting layers to non-trainable)\n    # optimizer: rmsprop\n    # loss: categorical crossentropy\n    # metrics: accuracy\n  \n## YOUR CODE HERE",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Train model",
   "metadata": {
    "id": "29DxiSHbJkbh",
    "colab_type": "text",
    "cell_id": "00029-32c95200-2015-473d-a1ce-f186216facea",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Train the model on the new, histological, data",
   "metadata": {
    "id": "XsYo9vGeeMBM",
    "colab_type": "text",
    "cell_id": "00030-8878fece-55d7-45f8-9068-0ad1fb8f9431",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GXtYUBTOeWl6",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00031-e8922a04-2c26-4910-8516-fa95a9d79bbb",
    "deepnote_cell_type": "code"
   },
   "source": "# Convert all of our training and validation ('test') data to TensorFlow data\n# This prevents the training algorithm from needing to make a *copy* of your\n# numpy arrays, which would EAT UP SOO MUCH RAM!\n#\n# It also accelerates training a bit because there is no data-conversion step\ntrain_images_tf = tf.constant(train_images, dtype=tf.float16)\ntest_images_tf = tf.constant(test_images)\ndel train_images, test_images\n\ntrain_labels_tf = tf.constant(train_labels, dtype=tf.float16)\ntest_labels_tf = tf.constant(test_labels)\ndel train_labels, test_labels",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Train model",
   "metadata": {
    "id": "2J-uBwdWerN5",
    "colab_type": "text",
    "cell_id": "00032-1858e45f-fb35-453c-b5ef-7350a22c507a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vpKK3afvJkbi",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00033-c005cb49-8516-41aa-b63e-192dd9daf572",
    "deepnote_cell_type": "code"
   },
   "source": "# This function is called after each epoch\n# (It will ensure that your training process does not consume all available RAM)\nclass garbage_collect_callback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    gc.collect()\n\n# Time how long it takes the model to train for these epochs\nstart_time = time.time()\n\n# Perform the training method\nhistory = model.fit(train_images_tf,\n                    train_labels_tf,\n                    batch_size=64,\n                    epochs= 50,\n                    verbose=1,\n                    validation_data=(test_images_tf, test_labels_tf),\n                    callbacks = [garbage_collect_callback()])\n\nstop_time = time.time()\nprint(\"--- %s seconds ---\" % (stop_time - start_time))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Plot model train/validation accuracy and model train/validation loss",
   "metadata": {
    "id": "wPGyseHcJkbk",
    "colab_type": "text",
    "cell_id": "00034-b58012d7-9e62-4371-9360-ee69852d64dd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EXegpORpJkbl",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00035-cd47517c-d824-49a7-938c-a24095a5c40d",
    "deepnote_cell_type": "code"
   },
   "source": "print(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Make Predictions for Test Images",
   "metadata": {
    "id": "n8GR-I4mJkbp",
    "colab_type": "text",
    "cell_id": "00036-7267320e-67c6-412f-8f16-d9c5825ca890",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QucAFGw1Jkbq",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00037-0006f603-82be-43f2-a530-f542081927c7",
    "deepnote_cell_type": "code"
   },
   "source": "# Predict class of test each test\npredictions = model.predict(test_images_tf, verbose=True)\n\n# Convert the predictions and true labels into category numbers\ntest_true_labels = test_labels_tf.numpy().argmax(axis=1)\ntest_pred_labels = predictions.argmax(axis=1)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rHx3kt6GJkbs",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00038-79eb9e20-a1db-4b41-9b6a-67780e376c7b",
    "deepnote_cell_type": "code"
   },
   "source": "# Plot a set of test images, along with predicted labels and true labels\nplt.figure(figsize=(16,20))\nfor ii in range(0, 16):\n    # Activate subplot and display image\n    plt.subplot(4,4,ii+1)\n    plt.imshow(test_images_tf[ii+100,:,:,:].numpy().astype(np.float))\n\n    # Turn off axes\n    plt.axis('off')\n\n    # Add annotaiton\n    plt.title('expected : ' + label_to_str[test_true_labels[ii+100]]\n              + '\\npredicted : ' + label_to_str[test_pred_labels[ii+100]])\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Accuracy",
   "metadata": {
    "id": "ntDR6ZWYiEgN",
    "colab_type": "text",
    "cell_id": "00039-d0e7da00-f8a6-4c98-a62e-dafa30366092",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yRrEEXx9iGZK",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00040-afb6a05c-929b-4e48-9b7f-3e1deb3ac9b4",
    "deepnote_cell_type": "code"
   },
   "source": "acc = accuracy_score(test_true_labels, test_pred_labels)\nprint(f'Model Accuracy: {acc:.2%}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Confusion matrix",
   "metadata": {
    "id": "Hpj8PtZuiIAU",
    "colab_type": "text",
    "cell_id": "00041-9077ddd7-488f-404a-826d-90210bff1097",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pp7rdgkciJJJ",
    "colab_type": "code",
    "colab": {},
    "cell_id": "00042-601202e6-e80a-4f13-a3cd-4fd037bebb15",
    "deepnote_cell_type": "code"
   },
   "source": "conf_mat = confusion_matrix(test_true_labels, test_pred_labels)\n\n# Generate a new figure\nplt.figure(figsize=(10,10))\n\n# Display the confusion matrix\nplt.imshow(conf_mat, cmap='hot', interpolation='nearest')\n\n# Add some anotation for the plot\nplt.colorbar()\nplt.xlabel('True label')\nplt.ylabel('Predicted label')\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### To-do:\n\nContinue playing around with preprocessing (image size) and the model (added layers, freezing layers, optimizer, # epochs) and see their effects on the accuracy. Doing this may help you for the challenge problem :O",
   "metadata": {
    "id": "XFwpTudBiMdv",
    "colab_type": "text",
    "cell_id": "00043-033f3a41-d186-4290-a61c-67a0c6ea209c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=8ceab382-16c2-4c1c-bf27-39206ff9a894' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "03_TransferLearning_Exercises.ipynb",
   "provenance": []
  },
  "deepnote_notebook_id": "3ee63698-adcb-46b5-9ef7-b6d264578fc8",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}